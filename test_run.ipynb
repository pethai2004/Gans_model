{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ad08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext tensorboard\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf98992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BaseGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d705ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def RUN(trainer, var):\n",
    "    with tf.GradientTape() as tape:\n",
    "        L = tf.reduce_sum(trainer.G.forward_model(np.random.rand(1, 200)))\n",
    "    g = tape.gradient(L, var)\n",
    "    trainer.G_opt.apply_gradients(zip(g, var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0877f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator_v0(BaseGenerator):\n",
    "    \n",
    "    def __init__(self, img_size=(32, 32), targ_img_size=(128, 128), z_dim=128, seed=1010, strategy_scope=None, name=\"GEN01\",\n",
    "                up_type=\"deconv\", apply_resize=False):\n",
    "        '''\n",
    "        Input : \n",
    "            image_size : (tuple) target image size at starting training (before first extent model).\n",
    "            targ_img_size : (tuple) target image size at the end of training.\n",
    "            apply_resize : (bool) whether to resize the img_size to the targ_img_size if is not equal in the forward call.\n",
    "        '''\n",
    "\n",
    "        super(Generator_v0, self).__init__(img_size, targ_img_size, z_dim, seed, strategy_scope, name)\n",
    "        assert up_type in [\"deconv\", \"upsample\"], \"up_type must be one of deconv or upsample\"\n",
    "        assert img_size[0] == img_size[1], \"img_size must be square\"\n",
    "        assert targ_img_size[0] == targ_img_size[1], \"targ_img_size must be square\"\n",
    "        self.up_type = up_type\n",
    "        self.blocks = []\n",
    "        self.cur_img_size = img_size\n",
    "        self.apply_resize = apply_resize\n",
    "        self.blocks = []\n",
    "        self.suff = NameCaller()\n",
    "        self.optimizer = None\n",
    "        self.cur_trainable = None # use just for creation of Variable that hold outside of tf.function\n",
    "\n",
    "    def _initialize_base(self):\n",
    "        '''Initialize model for based layer map and conv'''\n",
    "        inputer = keras.layers.Input(shape=(self.z_dim, ), name=\"BaseInput\"+self.suff.n)\n",
    "        x1 = latent_mapping(inputer, num_layers=self.configbuild[\"num_layers\"], units=self.configbuild[\"dense_units\"],\n",
    "                out_units=self.configbuild[\"out_units\"], activation=self.configbuild[\"dense_act\"], initializer=self.initializer, max_norm=None, \n",
    "                batch_norm=True, namemap=\"map_z\", name_suffix=\"\")\n",
    "        \n",
    "        x1 = keras.layers.Reshape(target_shape=(1, 1, self.configbuild[\"out_units\"]), name='reshape_mapper'+self.suff.n)(x1)\n",
    "\n",
    "        if self.up_type == \"deconv\":\n",
    "            for i_layer in self.configbuild[\"BaseFilters\"]:\n",
    "                x1 = get_deconv(x1, i_layer, self.configbuild[\"kernel_size\"], strides=(2, 2), padding=\"same\", activation=self.configbuild[\"conv_act\"], \n",
    "                        initializer=self.initializer, add_noise=None, max_norm=None, batch_norm=True, name_suffix=self.suff.n)\n",
    "        \n",
    "        elif self.up_type == \"upsample\":\n",
    "            for i_layer in self.configbuild[\"BaseFilters\"]:\n",
    "                x1 = get_upsampling(x1, i_layer, self.configbuild[\"kernel_size\"], strides=(1, 1), padding=\"same\", up_size=(2, 2), \n",
    "                activation=self.configbuild[\"conv_act\"], interpolation=\"nearest\", initializer=self.initializer, add_noise=None, batch_norm=True, \n",
    "                                    name_suffix=self.suff.n)\n",
    "                \n",
    "        outputer = keras.layers.Conv2D(3, self.configbuild[\"kernel_size\"], strides=(1, 1), padding=\"same\", activation=self.configbuild[\"out_act\"], use_bias=True,\n",
    "                        name=\"BaseOutConv\")(x1)\n",
    "        assert tf.reduce_prod(x1.shape[1:-1]) == tf.reduce_prod(self.img_size), \"mapping blocks do not correctly output shape, \"\\\n",
    "        \"must provide filters with length of {}\".format(int(np.log2(self.img_size[0])))\n",
    "        self.blocks.append(keras.Model(inputer, outputer, name=\"BaseGeneratorModel\"))\n",
    "\n",
    "    def _extend_model(self, filters=400, up=(1, 1), noise=False):\n",
    "        '''extent one convolutional blocks with either deconvolution or upsampling'''\n",
    "        if self.cur_img_size == self.targ_img_size:\n",
    "            assert (up == (1, 1) and filters==3), \"extent model cannot be called when image size is equal to target image size\"\n",
    "\n",
    "        if not self.initialized:\n",
    "            raise ValueError(\"Generator model not initialized\")\n",
    "        inputer = keras.layers.Input(shape=self.blocks[-1].output_shape[1:], name=\"ExtendedInput\"+self.suff.n)\n",
    "        self.cur_img_size = (self.cur_img_size[0] * up[0], self.cur_img_size[0] * up[1])\n",
    "        last_shape = self.blocks[-1].layers[-1].output_shape\n",
    "        \n",
    "        if self.up_type == \"deconv\":\n",
    "            outputer = get_deconv(inputer, filters, self.configbuild[\"kernel_size\"], strides=up, padding=\"same\",\n",
    "                activation=self.configbuild[\"conv_act\"], initializer=self.initializer, add_noise=noise, max_norm=None, batch_norm=True, name_suffix=self.suff.n)\n",
    "        \n",
    "        elif self.up_type ==\"upsample\":\n",
    "            outputer = get_upsampling(inputer, filters, self.configbuild[\"kernel_size\"], strides=(1, 1), padding=\"same\", \n",
    "                up_size=up, activation=self.configbuild[\"conv_act\"], interpolation=\"nearest\", initializer=self.initializer, \n",
    "                add_noise=noise, batch_norm=True, name_suffix=self.suff.n)\n",
    "            \n",
    "        if up[0] > 1:\n",
    "            outputer = keras.layers.Conv2D(3, (5, 5), strides=(1, 1), padding=\"same\", activation=None, use_bias=True, name=\"Conv\"+self.suff.n)(outputer)\n",
    "            assert tf.reduce_prod(outputer.shape[1:-1]) == tf.reduce_prod(self.cur_img_size), \"extended model do not correctly output shape\"\\\n",
    "            \"must provide filters with length of {}\".format(int(np.log2(self.cur_img_size[0])))\n",
    "        self.blocks.append(keras.Model(inputer, outputer))\n",
    "        print(\"extended model from size of\", last_shape, \"to\", outputer.shape)\n",
    "    \n",
    "    def _auto_extend(self):\n",
    "        '''extend model automatically with arbitrary number of blocks provided in configbuild'''\n",
    "        for ifilt in next(iter(self.configbuild[\"filters\"]))[:-1]:\n",
    "            self._extend_model(ifilt, up=(1, 1))\n",
    "        self._extend_model(next(iter(self.configbuild[\"filters\"]))[-1], up=(2, 2), noise=get_noise_out)\n",
    "\n",
    "    def set_mapping_trainable(self, trainable=True, prefix='map_z'):\n",
    "        for ly in self.get_flat_layers():\n",
    "            if prefix in ly.name:\n",
    "                ly.trainable = trainable\n",
    "                print(\"set trainable_variables of layers {} to {}\".format(ly.name, trainable))\n",
    "    \n",
    "    def set_joint_trainable(self, trainable=True, not_prefix='map_z'):\n",
    "        for ly in self.get_flat_layers():\n",
    "            if not_prefix not in ly.name and ly.trainable_variables != []:  \n",
    "                ly.trainable = trainable\n",
    "                print(\"set trainable_variables of layers {} to {}\".format(ly.name, trainable))\n",
    "                \n",
    "    def forward_model(self, inputs, training=True): ############ use get_model instead of calling function sequentially, this may result in async weights update\n",
    "        for bk in self.blocks:\n",
    "            inputs = bk(inputs)\n",
    "        if (self.apply_resize and self.cur_img_size != self.targ_img_size):\n",
    "            inputs = tf.image.resize(inputs, self.targ_img_size, method=\"nearest\")\n",
    "        return tf.cast(inputs, dtype=tf.float32)\n",
    "    \n",
    "    def get_model(self, get_with_functional=False, with_scope=False):\n",
    "        assert not get_with_functional\n",
    "        assert self.extendable, \"This Generator is not extendable\"\n",
    "        U_layers = self.blocks if get_with_functional else self.get_flat_layers()\n",
    "        inputer = keras.layers.Input(shape=(self.z_dim), name='BaseInput000')\n",
    "        xi = U_layers[1](inputer)\n",
    "        for J_layers in U_layers[2:]:\n",
    "            xi = J_layers(xi)\n",
    "        if with_scope: # this actually returns Model with scope since it is sync for all tf.Variable, Just in case \n",
    "            with self.strategy_scope.scope():\n",
    "                return keras.Model(inputer, xi)\n",
    "        else:\n",
    "            return keras.Model(inputer, xi)\n",
    "    \n",
    "    def get_flat_layers(self):\n",
    "        lys = []\n",
    "        for msl in self.blocks:\n",
    "            for ms in msl.layers: \n",
    "                lys.append(ms)\n",
    "        return lys\n",
    "    \n",
    "    def print_config_layers(self):\n",
    "        for i_layer in self.get_flat_layers():\n",
    "            print(\"Name:{} - in_shape:{} - out_shape:{} - trainable:{} - scope{}\".format(\n",
    "            i_layer.name, i_layer.input_shape, i_layer.output_shape, i_layer.trainable, i_layer.name_scope()))\n",
    "    \n",
    "    def get_trainable(self):\n",
    "        # self.cur_trainable = self.get_model().trainable_variables\n",
    "        self.cur_trainable = []\n",
    "        for iu in self.get_flat_layers():\n",
    "            for s in iu.trainable_variables:\n",
    "                self.cur_trainable.append(s)\n",
    "        return self.cur_trainable\n",
    "\n",
    "    def update_params(self, grads):\n",
    "        assert self.optimizer is not None, \"optimizer is not provided\"\n",
    "        if self.cur_trainable is None: # this must not run in the context of tf.function since it create new variables\n",
    "            self.get_trainable()\n",
    "        self.optimizer.apply_gradients(zip(grads, self.cur_trainable))\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.get_model().save(path)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fd9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "44a9cdcbdccbf05a880e90d2e6fe72470baab4d1b82472d890be0596ed887a6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
